#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¯¼å…¥èµ„æºè„šæœ¬ï¼ˆç»ˆæç‰ˆï¼‰
åŠŸèƒ½ï¼šä»new_resources.txtæ‰¹é‡å¯¼å…¥èµ„æºï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼é€æ¡æå–
è®¾è®¡åŸåˆ™ï¼šä¸¥æ ¼åŒ¿åï½œåˆè§„ä¼˜å…ˆï½œå¼ºå¤§å®¹é”™
"""
import json
import os
import re
from datetime import datetime

def extract_resources_regex():
    """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼é€æ¡æå–èµ„æº"""
    try:
        with open("new_resources.txt", 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ¯ä¸ªèµ„æºå¯¹è±¡
        # åŒ¹é…æ¨¡å¼ï¼š{"title":"...","type":"...","url":"...","description":"...","verified_by":[...],"last_updated":"...","compliance_level":"..."}
        pattern = r'\{[^{}]*?"title"[^{}]*?\}'
        matches = re.findall(pattern, content)
        
        resources = []
        for match in matches:
            try:
                # å°è¯•è§£ææ¯ä¸ªåŒ¹é…çš„å¯¹è±¡
                obj = json.loads(match)
                if "title" in obj and "url" in obj:
                    resources.append(obj)
            except:
                continue
        
        if resources:
            print(f"âœ… æ­£åˆ™æå–æˆåŠŸï¼Œè¯»å– {len(resources)} æ¡èµ„æº")
            return resources
        else:
            print("âŒ æ­£åˆ™æå–å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨è§£æ...")
            return manual_parse(content)
    
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        return None

def manual_parse(content):
    """æ‰‹åŠ¨è§£æï¼ˆæœ€åçš„æ‰‹æ®µï¼‰"""
    # æå–JSONæ•°ç»„éƒ¨åˆ†
    json_start = content.find('[')
    if json_start == -1:
        return None
    
    json_content = content[json_start:]
    
    # é€è¡Œå¤„ç†ï¼Œæå–å…³é”®ä¿¡æ¯
    resources = []
    lines = json_content.split('\n')
    
    current_resource = {}
    for line in lines:
        line = line.strip()
        
        # æå–title
        if '"title"' in line:
            match = re.search(r'"title"\s*:\s*"([^"]*)"', line)
            if match:
                current_resource['title'] = match.group(1)
        
        # æå–type
        if '"type"' in line:
            match = re.search(r'"type"\s*:\s*"([^"]*)"', line)
            if match:
                current_resource['type'] = match.group(1)
        
        # æå–url
        if '"url"' in line:
            match = re.search(r'"url"\s*:\s*"([^"]*)"', line)
            if match:
                current_resource['url'] = match.group(1)
        
        # æå–description
        if '"description"' in line:
            match = re.search(r'"description"\s*:\s*"([^"]*)"', line)
            if match:
                current_resource['description'] = match.group(1)
        
        # æå–verified_by
        if '"verified_by"' in line:
            match = re.search(r'"verified_by"\s*:\s*\[([^\]]*)\]', line)
            if match:
                verified_str = match.group(1)
                current_resource['verified_by'] = [v.strip('"') for v in verified_str.split(',')]
        
        # æå–last_updated
        if '"last_updated"' in line:
            match = re.search(r'"last_updated"\s*:\s*"([^"]*)"', line)
            if match:
                current_resource['last_updated'] = match.group(1)
        
        # æå–compliance_level
        if '"compliance_level"' in line:
            match = re.search(r'"compliance_level"\s*:\s*"([^"]*)"', line)
            if match:
                current_resource['compliance_level'] = match.group(1)
        
        # å¦‚æœé‡åˆ°å¯¹è±¡ç»“æŸï¼Œä¿å­˜å½“å‰èµ„æº
        if line.endswith('},') or line.endswith('}'):
            if 'title' in current_resource and 'url' in current_resource:
                resources.append(current_resource.copy())
            current_resource = {}
    
    if resources:
        print(f"âœ… æ‰‹åŠ¨è§£ææˆåŠŸï¼Œè¯»å– {len(resources)} æ¡èµ„æº")
        return resources
    else:
        return None

def load_main_index():
    """è¯»å–ä¸»ç´¢å¼•"""
    try:
        with open("content_index.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if "index" in data:
            resources = data["index"]
        elif "resources" in data:
            resources = data["resources"]
        else:
            resources = []
        
        print(f"ğŸ“Š å½“å‰èµ„æºæ•°: {len(resources)}æ¡")
        return data, resources
    
    except Exception as e:
        print(f"âŒ è¯»å–ä¸»ç´¢å¼•å¤±è´¥: {e}")
        return None, None

def convert_to_standard_format(new_resource, index):
    """è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼ˆåŒ¿åæ€§å¼ºåŒ–ï¼‰"""
    standard_resource = {
        "title": new_resource.get("title", ""),
        "content_type": new_resource.get("type", "èµ„æºåˆ†äº«"),
        "summary": new_resource.get("description", ""),
        "direct_link": new_resource.get("url", ""),
        "verified_by": new_resource.get("verified_by", []),
        "last_updated": new_resource.get("last_updated", datetime.now().strftime("%Y-%m-%d")),
        "compliance_level": new_resource.get("compliance_level", "å¾…éªŒè¯"),
        "keywords": "",
        "compliance_status": "é€šè¿‡",
        "version": "1.0.0",
        "compliance_hash": "batch_import",
        "content_id": f"ai_item_{index:04d}",
        "full_content_anchor": f"ai_item_{index:04d}"
    }
    
    return standard_resource

def merge_resources(existing_resources, new_resources):
    """åˆå¹¶èµ„æºï¼ˆå»é‡ï¼‰"""
    existing_urls = {r.get("direct_link", r.get("url", "")) for r in existing_resources}
    
    merged = existing_resources.copy()
    added_count = 0
    duplicate_count = 0
    
    for new_res in new_resources:
        url = new_res.get("url", "")
        if url and url not in existing_urls:
            standard_res = convert_to_standard_format(new_res, len(merged) + 1)
            merged.append(standard_res)
            existing_urls.add(url)
            added_count += 1
        else:
            duplicate_count += 1
    
    print(f"âœ… æ–°å¢èµ„æº: {added_count}æ¡")
    if duplicate_count > 0:
        print(f"âš ï¸  é‡å¤è·³è¿‡: {duplicate_count}æ¡")
    
    return merged

def save_main_index(data, resources):
    """ä¿å­˜ä¸»ç´¢å¼•"""
    backup_path = "content_index.json.bak"
    if os.path.exists("content_index.json"):
        with open("content_index.json", 'r', encoding='utf-8') as f:
            backup_content = f.read()
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(backup_content)
        print(f"ğŸ’¾ å·²å¤‡ä»½åŸæ–‡ä»¶: {backup_path}")
    
    if "index" in data:
        data["index"] = resources
    elif "resources" in data:
        data["resources"] = resources
    
    data["total_count"] = len(resources)
    
    with open("content_index.json", 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ä¸»ç´¢å¼•å·²æ›´æ–°: {len(resources)}æ¡èµ„æº")

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("æ‰¹é‡å¯¼å…¥èµ„æºè„šæœ¬ï¼ˆç»ˆæç‰ˆï¼‰")
    print("="*70)
    print("ğŸ”’ åŒ¿åæ€§ä¿éšœï¼šä¸ç”Ÿæˆä»»ä½•å¯è¿½è¸ªå­—æ®µ")
    print("="*70)
    
    print("\n[æ­¥éª¤1] è¯»å–æ–°èµ„æº...")
    new_resources = extract_resources_regex()
    if not new_resources:
        return
    
    print("\n[æ­¥éª¤2] è¯»å–ä¸»ç´¢å¼•...")
    data, existing_resources = load_main_index()
    if data is None:
        return
    
    print("\n[æ­¥éª¤3] åˆå¹¶èµ„æºï¼ˆå»é‡ï¼‰...")
    merged_resources = merge_resources(existing_resources, new_resources)
    
    print("\n[æ­¥éª¤4] ä¿å­˜ä¸»ç´¢å¼•...")
    save_main_index(data, merged_resources)
    
    print("\n" + "="*70)
    print("âœ¨ æ‰¹é‡å¯¼å…¥å®Œæˆï¼")
    print("="*70)
    print(f"ğŸ“Š åŸæœ‰èµ„æº: {len(existing_resources)}æ¡")
    print(f"ğŸ“Š æ–°å¢èµ„æº: {len(merged_resources) - len(existing_resources)}æ¡")
    print(f"ğŸ“Š æ€»èµ„æºæ•°: {len(merged_resources)}æ¡")
    print("="*70)
    print("\nğŸ’¡ åç»­æ“ä½œå»ºè®®ï¼š")
    print("  1. ã€å¿…é¡»ã€‘è¿è¡ŒåŒ¿åä¼˜åŒ–è„šæœ¬ï¼špython scripts/optimize_anonymous.py")
    print("  2. æäº¤ä»£ç ï¼šgit add . && git commit -m 'add: æ‰¹é‡å¯¼å…¥èµ„æº' && git push")
    print("\n")

if __name__ == "__main__":
    main()
