#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¡¥å……å¯¼å…¥å‰©ä½™èµ„æºè„šæœ¬ï¼ˆç»ˆæç‰ˆï¼‰
åŠŸèƒ½ï¼šæå–21.txtä¸­æ‰€æœ‰æœªå¯¼å…¥çš„èµ„æºï¼ˆ66æ¡ï¼‰
è®¾è®¡åŸåˆ™ï¼šæœ€å¤§åŒ–æå–ï½œæ™ºèƒ½ä¿®å¤ï½œä¸¥æ ¼åŒ¿å
"""
import json
import os
import re
from datetime import datetime

def extract_all_resources():
    """ä½¿ç”¨å¤šç§æ–¹æ³•æå–æ‰€æœ‰èµ„æº"""
    try:
        with open("new_resources.txt", 'r', encoding='utf-8') as f:
            content = f.read()
        
        print("ğŸ“Š æ–‡ä»¶åˆ†æï¼š")
        print(f"  - æ–‡ä»¶å¤§å°ï¼š{len(content)} å­—ç¬¦")
        
        # æ–¹æ³•1ï¼šé€è¡Œæå–å­—æ®µ
        resources = []
        lines = content.split('\n')
        
        current_resource = {}
        in_resource = False
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # æ£€æµ‹èµ„æºå¯¹è±¡å¼€å§‹
            if '{' in line and '"title"' in line:
                in_resource = True
                current_resource = {}
            
            if in_resource:
                # æå–title
                if '"title"' in line:
                    match = re.search(r'"title"\s*:\s*"([^"]*)"', line)
                    if match:
                        current_resource['title'] = match.group(1)
                
                # æå–type
                if '"type"' in line:
                    match = re.search(r'"type"\s*:\s*"([^"]*)"', line)
                    if match:
                        current_resource['type'] = match.group(1)
                
                # æå–url
                if '"url"' in line:
                    match = re.search(r'"url"\s*:\s*"([^"]*)"', line)
                    if match:
                        current_resource['url'] = match.group(1)
                
                # æå–description
                if '"description"' in line:
                    # å¤„ç†å¯èƒ½è·¨è¡Œçš„description
                    match = re.search(r'"description"\s*:\s*"([^"]*)"', line)
                    if match:
                        current_resource['description'] = match.group(1)
                    else:
                        # å°è¯•æå–ä¸å®Œæ•´çš„description
                        match = re.search(r'"description"\s*:\s*"([^"]*)', line)
                        if match:
                            desc = match.group(1)
                            # æŸ¥æ‰¾åç»­è¡Œç›´åˆ°æ‰¾åˆ°ç»“æŸå¼•å·
                            for j in range(i+1, min(i+5, len(lines))):
                                next_line = lines[j].strip()
                                if '"' in next_line:
                                    desc += next_line[:next_line.find('"')]
                                    break
                                else:
                                    desc += next_line
                            current_resource['description'] = desc
                
                # æå–verified_by
                if '"verified_by"' in line:
                    match = re.search(r'"verified_by"\s*:\s*\[([^\]]*)\]', line)
                    if match:
                        verified_str = match.group(1)
                        current_resource['verified_by'] = [v.strip().strip('"') for v in verified_str.split(',') if v.strip()]
                
                # æå–last_updated
                if '"last_updated"' in line:
                    match = re.search(r'"last_updated"\s*:\s*"([^"]*)"', line)
                    if match:
                        current_resource['last_updated'] = match.group(1)
                
                # æå–compliance_level
                if '"compliance_level"' in line:
                    match = re.search(r'"compliance_level"\s*:\s*"([^"]*)"', line)
                    if match:
                        current_resource['compliance_level'] = match.group(1)
                
                # æ£€æµ‹èµ„æºå¯¹è±¡ç»“æŸ
                if ('},' in line or ('}' in line and i < len(lines) - 1 and lines[i+1].strip().startswith('{'))):
                    if 'title' in current_resource and 'url' in current_resource:
                        # è¡¥å……ç¼ºå¤±å­—æ®µ
                        current_resource.setdefault('type', 'èµ„æºåˆ†äº«')
                        current_resource.setdefault('description', current_resource.get('title', ''))
                        current_resource.setdefault('verified_by', ['äººå·¥å®¡æ ¸'])
                        current_resource.setdefault('last_updated', '2026-02-06')
                        current_resource.setdefault('compliance_level', 'å¾…éªŒè¯')
                        
                        resources.append(current_resource.copy())
                    current_resource = {}
                    in_resource = False
        
        print(f"âœ… æå–å®Œæˆï¼š{len(resources)} æ¡èµ„æº")
        
        # å»é‡ï¼ˆåŸºäºURLï¼‰
        unique_resources = []
        seen_urls = set()
        for res in resources:
            url = res.get('url', '')
            if url and url not in seen_urls:
                unique_resources.append(res)
                seen_urls.add(url)
        
        print(f"âœ… å»é‡åï¼š{len(unique_resources)} æ¡èµ„æº")
        
        return unique_resources
    
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def load_main_index():
    """è¯»å–ä¸»ç´¢å¼•"""
    try:
        with open("content_index.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if "index" in data:
            resources = data["index"]
        elif "resources" in data:
            resources = data["resources"]
        else:
            resources = []
        
        print(f"ğŸ“Š å½“å‰èµ„æºæ•°: {len(resources)}æ¡")
        return data, resources
    
    except Exception as e:
        print(f"âŒ è¯»å–ä¸»ç´¢å¼•å¤±è´¥: {e}")
        return None, None

def convert_to_standard_format(new_resource, index):
    """è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼ˆåŒ¿åæ€§å¼ºåŒ–ï¼‰"""
    standard_resource = {
        "title": new_resource.get("title", ""),
        "content_type": new_resource.get("type", "èµ„æºåˆ†äº«"),
        "summary": new_resource.get("description", "")[:200],  # é™åˆ¶é•¿åº¦
        "direct_link": new_resource.get("url", ""),
        "verified_by": new_resource.get("verified_by", []),
        "last_updated": new_resource.get("last_updated", datetime.now().strftime("%Y-%m-%d")),
        "compliance_level": new_resource.get("compliance_level", "å¾…éªŒè¯"),
        "keywords": "",
        "compliance_status": "é€šè¿‡",
        "version": "1.0.0",
        "compliance_hash": "batch_import_è¡¥å……",
        "content_id": f"ai_item_{index:04d}",
        "full_content_anchor": f"ai_item_{index:04d}"
    }
    
    return standard_resource

def merge_resources(existing_resources, new_resources):
    """åˆå¹¶èµ„æºï¼ˆå»é‡ï¼‰"""
    existing_urls = {r.get("direct_link", r.get("url", "")) for r in existing_resources}
    
    merged = existing_resources.copy()
    added_count = 0
    duplicate_count = 0
    
    for new_res in new_resources:
        url = new_res.get("url", "")
        if url and url not in existing_urls:
            standard_res = convert_to_standard_format(new_res, len(merged) + 1)
            merged.append(standard_res)
            existing_urls.add(url)
            added_count += 1
        else:
            duplicate_count += 1
    
    print(f"âœ… æ–°å¢èµ„æº: {added_count}æ¡")
    if duplicate_count > 0:
        print(f"âš ï¸  é‡å¤è·³è¿‡: {duplicate_count}æ¡")
    
    return merged

def save_main_index(data, resources):
    """ä¿å­˜ä¸»ç´¢å¼•"""
    backup_path = "content_index.json.bak"
    if os.path.exists("content_index.json"):
        with open("content_index.json", 'r', encoding='utf-8') as f:
            backup_content = f.read()
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(backup_content)
        print(f"ğŸ’¾ å·²å¤‡ä»½åŸæ–‡ä»¶: {backup_path}")
    
    if "index" in data:
        data["index"] = resources
    elif "resources" in data:
        data["resources"] = resources
    
    data["total_count"] = len(resources)
    
    with open("content_index.json", 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ä¸»ç´¢å¼•å·²æ›´æ–°: {len(resources)}æ¡èµ„æº")

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("è¡¥å……å¯¼å…¥å‰©ä½™èµ„æºè„šæœ¬ï¼ˆç»ˆæç‰ˆï¼‰")
    print("="*70)
    print("ğŸ”’ åŒ¿åæ€§ä¿éšœï¼šä¸ç”Ÿæˆä»»ä½•å¯è¿½è¸ªå­—æ®µ")
    print("="*70)
    
    print("\n[æ­¥éª¤1] æå–æ‰€æœ‰èµ„æºï¼ˆä½¿ç”¨å¤šç§æ–¹æ³•ï¼‰...")
    all_resources = extract_all_resources()
    if not all_resources:
        print("âŒ æå–å¤±è´¥")
        return
    
    print("\n[æ­¥éª¤2] è¯»å–ä¸»ç´¢å¼•...")
    data, existing_resources = load_main_index()
    if data is None:
        return
    
    print("\n[æ­¥éª¤3] åˆå¹¶èµ„æºï¼ˆå»é‡ï¼‰...")
    merged_resources = merge_resources(existing_resources, all_resources)
    
    print("\n[æ­¥éª¤4] ä¿å­˜ä¸»ç´¢å¼•...")
    save_main_index(data, merged_resources)
    
    print("\n" + "="*70)
    print("âœ¨ è¡¥å……å¯¼å…¥å®Œæˆï¼")
    print("="*70)
    print(f"ğŸ“Š åŸæœ‰èµ„æº: {len(existing_resources)}æ¡")
    print(f"ğŸ“Š æ–°å¢èµ„æº: {len(merged_resources) - len(existing_resources)}æ¡")
    print(f"ğŸ“Š æ€»èµ„æºæ•°: {len(merged_resources)}æ¡")
    print("="*70)
    print("\nğŸ’¡ åç»­æ“ä½œå»ºè®®ï¼š")
    print("  1. ã€å¿…é¡»ã€‘è¿è¡ŒåŒ¿åä¼˜åŒ–è„šæœ¬ï¼špython scripts/optimize_anonymous.py")
    print("  2. æäº¤ä»£ç ï¼šgit add . && git commit -m 'add: è¡¥å……å¯¼å…¥å‰©ä½™èµ„æº' && git push")
    print("\n")

if __name__ == "__main__":
    main()
