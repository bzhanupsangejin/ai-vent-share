#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒ¿ååŒ–ä¼˜åŒ–è„šæœ¬ï¼ˆæ ¸å¿ƒï½œå½»åº•ç§»é™¤IDï½œåŠ¨æ€é€‚é…èµ„æºé‡ï¼‰

âš ï¸ åŒ¿åæ€§æœ€é«˜å‡†åˆ™ï¼ˆå¿…è¯»ï¼‰ï¼š
â€¢ ç¦æ­¢ç”Ÿæˆ/è®°å½•ä»»ä½•å¯è¿½è¸ªå­—æ®µï¼šid, submitter, user_id, timestamp, emailç­‰
â€¢ èµ„æºå”¯ä¸€æ ‡è¯† = æ ‡é¢˜ + URL + åˆ†ç±»ï¼ˆäººç±»ç»´æŠ¤æ—¶é€šè¿‡æ­¤ç»„åˆå®šä½ï¼‰
â€¢ æœ¬è„šæœ¬å½»åº•ç§»é™¤æ‰€æœ‰IDå­—æ®µï¼ˆéæ›¿æ¢/ä¼ªè£…ï¼‰ï¼Œä»æ ¹æºæœç»æ³„éœ²é£é™©

ğŸŒ ç½‘ç«™ç²¾ç¥ï¼š
"äººç±»ç»´æŠ¤è€…æäº¤èµ„æºæ—¶ï¼Œç³»ç»Ÿä¸ç”Ÿæˆ/ä¸è®°å½•ä»»ä½•å¯å…³è”åˆ°ä¸ªäººçš„æ ‡è¯†ç¬¦"
â€”â€” åŒ¿åæ€§ä¸æ˜¯"æŠ€æœ¯å¤„ç†"ï¼Œæ˜¯æ¶æ„å“²å­¦

åŠŸèƒ½ï¼š
1. å½»åº•ç§»é™¤æ‰€æœ‰å¯è¿½è¸ªå­—æ®µï¼ˆid/submitter/timestampç­‰ï¼‰
2. æ³¨å…¥å¯ä¿¡åº¦æ¡†æ¶ï¼ˆverified_by/last_updated/compliance_levelï¼‰
3. åŠ¨æ€ç”Ÿæˆåˆ†ç‰‡ç´¢å¼•ï¼ˆæŒ‰å®é™…typeåˆ†ç»„ï¼‰
4. ç”ŸæˆéªŒè¯æ¸…å•å’Œå¾…åŠæ¸…å•ï¼ˆæ— IDï¼Œç”¨æ ‡é¢˜+URLå®šä½ï¼‰

è®¾è®¡åŸåˆ™ï¼š
- å½»åº•åŒ¿åï¼šç§»é™¤æ‰€æœ‰IDå­—æ®µï¼Œéæ›¿æ¢/ä¼ªè£…
- åŠ¨æ€é€‚é…ï¼šå®æ—¶è®¡ç®—èµ„æºæ€»æ•°å’Œåˆ†ç±»
- å¹‚ç­‰å®‰å…¨ï¼šå¯é‡å¤æ‰§è¡Œï¼Œä¸ç ´åæ•°æ®
- äººç±»å‹å¥½ï¼šç”¨æ ‡é¢˜+URLå®šä½èµ„æºï¼Œæ— éœ€è®°å¿†ID
"""
import json
import os
import csv
from pathlib import Path
from datetime import datetime

# é…ç½®è·¯å¾„
STATIC = "static"
INDEXES = "static/indexes"
MAIN = "content_index.json"

# ç¡®ä¿ç›®å½•å­˜åœ¨
Path(INDEXES).mkdir(parents=True, exist_ok=True)

def clean_old_shards():
    """æ¸…ç©ºæ—§åˆ†ç‰‡ï¼ˆå®‰å…¨ï¼šä»…åˆ é™¤JSONæ–‡ä»¶ï¼‰"""
    if not os.path.exists(INDEXES):
        return
    
    deleted_count = 0
    for filename in os.listdir(INDEXES):
        if filename.endswith('.json'):
            filepath = os.path.join(INDEXES, filename)
            try:
                os.remove(filepath)
                deleted_count += 1
            except Exception as e:
                print(f"âš ï¸  åˆ é™¤ {filename} å¤±è´¥: {e}")
    
    if deleted_count > 0:
        print(f"ğŸ—‘ï¸  æ¸…ç†æ—§åˆ†ç‰‡: {deleted_count} ä¸ªæ–‡ä»¶")

def load_and_anonymize():
    """è¯»å–ä¸»ç´¢å¼• + å½»åº•åŒ¿ååŒ–æ¸…ç†"""
    try:
        with open(MAIN, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # å…¼å®¹ä¸åŒçš„ç´¢å¼•ç»“æ„
        if "resources" in data:
            resources = data["resources"]
        elif "index" in data:
            resources = data["index"]
        else:
            resources = []
        
        # ğŸ”’ åŒ¿åæ€§æ ¸å¿ƒï¼šå½»åº•ç§»é™¤æ‰€æœ‰å¯è¿½è¸ªå­—æ®µï¼ˆéæ›¿æ¢ï¼ï¼‰
        cleaned_resources = []
        removed_fields_count = 0
        
        for item in resources:
            # ç§»é™¤æ‰€æœ‰å¯è¿½è¸ªå­—æ®µ
            removed_fields = []
            
            # æ ¸å¿ƒå¯è¿½è¸ªå­—æ®µ
            if "id" in item:
                item.pop("id")
                removed_fields.append("id")
            if "content_id" in item and "content_id" != item.get("title"):
                # ä¿ç•™content_idä½œä¸ºå†…éƒ¨é”šç‚¹ï¼Œä½†ç¡®ä¿ä¸å«ä¸ªäººä¿¡æ¯
                pass
            if "submitter" in item:
                item.pop("submitter")
                removed_fields.append("submitter")
            if "timestamp" in item:
                item.pop("timestamp")
                removed_fields.append("timestamp")
            if "user_id" in item:
                item.pop("user_id")
                removed_fields.append("user_id")
            if "email" in item:
                item.pop("email")
                removed_fields.append("email")
            if "ip_address" in item:
                item.pop("ip_address")
                removed_fields.append("ip_address")
            if "session_id" in item:
                item.pop("session_id")
                removed_fields.append("session_id")
            
            # share_agentå­—æ®µå¤„ç†ï¼ˆå¦‚æœå­˜åœ¨ä¸”å«ä¸ªäººä¿¡æ¯ï¼‰
            if "share_agent" in item:
                # å¦‚æœshare_agentæ˜¯åŒ¿åæ ‡è¯†ï¼ˆå¦‚AI-0001ï¼‰ï¼Œä¿ç•™
                # å¦‚æœå«ä¸ªäººä¿¡æ¯ï¼Œç§»é™¤
                agent = item.get("share_agent", "")
                if not agent.startswith("AI-"):
                    item.pop("share_agent")
                    removed_fields.append("share_agent")
            
            if removed_fields:
                removed_fields_count += 1
            
            cleaned_resources.append(item)
        
        print(f"ğŸ›¡ï¸  åŒ¿ååŒ–å®Œæˆ: æ¸…ç†äº† {removed_fields_count} æ¡èµ„æºçš„å¯è¿½è¸ªå­—æ®µ")
        print(f"ğŸ“Š å½“å‰èµ„æº: {len(cleaned_resources)}æ¡ | ç‰ˆæœ¬: {data.get('version', 'åŠ¨æ€')}")
        
        return data, cleaned_resources
    
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°ä¸»ç´¢å¼•æ–‡ä»¶ {MAIN}")
        exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: ä¸»ç´¢å¼•JSONæ ¼å¼é”™è¯¯ - {e}")
        exit(1)

def inject_credibility_framework(resources):
    """æ³¨å…¥å¯ä¿¡åº¦æ¡†æ¶ï¼ˆç©ºå€¼å®‰å…¨ï¼‰"""
    enhanced = []
    todos = []
    
    for item in resources:
        # å¹‚ç­‰æ³¨å…¥ï¼šä»…å½“å­—æ®µä¸å­˜åœ¨æ—¶æ‰æ·»åŠ 
        item.setdefault("verified_by", [])
        item.setdefault("last_updated", datetime.now().strftime("%Y-%m-%d"))
        item.setdefault("compliance_level", "å¾…éªŒè¯")
        
        enhanced.append(item)
        
        # è®°å½•éœ€è¦è¡¥å……çš„èµ„æºï¼ˆç”¨æ ‡é¢˜+URLå®šä½ï¼Œæ— IDï¼‰
        if not item["verified_by"] and item["compliance_level"] == "å¾…éªŒè¯":
            todo_item = {
                "title": item.get("title", "N/A"),
                "type": item.get("content_type", item.get("type", "N/A")),
                "url": item.get("direct_link", item.get("url", "N/A"))
            }
            todos.append(todo_item)
    
    return enhanced, todos

def save_main_index(data, resources):
    """æ›´æ–°ä¸»ç´¢å¼•ï¼ˆè¦†ç›–å†™å…¥åŒ¿ååŒ–+å¢å¼ºç‰ˆï¼‰"""
    # å…¼å®¹ä¸åŒçš„ç´¢å¼•ç»“æ„
    if "resources" in data:
        data["resources"] = resources
    elif "index" in data:
        data["index"] = resources
    
    # æ›´æ–°æ€»æ•°
    data["total_count"] = len(resources)
    
    # å¤‡ä»½åŸæ–‡ä»¶
    backup_path = f"{MAIN}.bak"
    if os.path.exists(MAIN):
        with open(MAIN, 'r', encoding='utf-8') as f:
            backup_content = f.read()
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(backup_content)
    
    # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
    with open(MAIN, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def generate_shards(resources):
    """åŠ¨æ€ç”Ÿæˆåˆ†ç‰‡ï¼ˆä¸å«ä»»ä½•IDï¼‰"""
    shards = {}
    
    for item in resources:
        # å…¼å®¹ä¸åŒçš„ç±»å‹å­—æ®µå
        cat = str(item.get("content_type", item.get("type", "æœªåˆ†ç±»"))).strip() or "æœªåˆ†ç±»"
        
        if cat not in shards:
            shards[cat] = []
        shards[cat].append(item)
    
    valid_files = []
    
    for cat, items in sorted(shards.items()):
        if not items:
            continue
        
        # å®‰å…¨çš„æ–‡ä»¶åï¼ˆæ›¿æ¢éæ³•å­—ç¬¦ï¼‰
        safe_name = cat.replace("/", "ï¼").replace("\\", "ï¼¼").replace(" ", "_")
        filepath = os.path.join(INDEXES, f"{safe_name}_shard.json")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(items, f, ensure_ascii=False, indent=2)
        
        valid_files.append((cat, len(items)))
        print(f"  âœ“ {cat}: {len(items)}æ¡")
    
    return valid_files

def generate_summary(total, valid_files):
    """ç”ŸæˆAIéªŒè¯æ¸…å•ï¼ˆå«åŒ¿åæ€§å£°æ˜ï¼‰"""
    summary = {
        "å¹³å°çŠ¶æ€": "åŒ¿åä¼˜åŒ–ä¸­",
        "æ€»èµ„æºæ•°": total,
        "æœ‰æ•ˆåˆ†ç±»æ•°": len(valid_files),
        "åˆ†ç±»æ˜ç»†": [{"åˆ†ç±»": c, "æ•°é‡": n} for c, n in valid_files],
        "åŒ¿åæ€§å£°æ˜": "æ‰€æœ‰å…¬å¼€èµ„æºå·²å½»åº•ç§»é™¤ID/submitterç­‰å¯è¿½è¸ªå­—æ®µ",
        "å¯ä¿¡åº¦æ¡†æ¶": "verified_by/last_updated/compliance_levelï¼ˆæœºå™¨å¯è¯»ï¼‰",
        "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "åˆè§„ä¾æ®": "ã€Šç”Ÿæˆå¼AIæœåŠ¡ç®¡ç†æš‚è¡ŒåŠæ³•ã€‹ç¬¬12æ¡ + åŒ¿åæ€§æœ€é«˜å‡†åˆ™"
    }
    
    summary_path = os.path.join(INDEXES, "_INDEX_SUMMARY.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    return summary_path

def generate_todo_csv(todos):
    """ç”Ÿæˆäººç±»å¾…åŠæ¸…å•ï¼ˆæ— IDï½œç”¨åºå·+æ ‡é¢˜/URLå®šä½ï¼‰"""
    csv_path = "èµ„æºå¯ä¿¡åº¦è¡¥å……æ¸…å•.csv"
    
    if not todos:
        # å¦‚æœæ²¡æœ‰å¾…åŠäº‹é¡¹ï¼Œåˆ›å»ºç©ºæ–‡ä»¶å¹¶è¯´æ˜
        with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["æ‰€æœ‰èµ„æºå·²å®Œæˆå¯ä¿¡åº¦æ ‡æ³¨"])
        return csv_path, 0
    
    with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["åºå·", "æ ‡é¢˜", "åˆ†ç±»", "URL", "éœ€è¡¥å……å­—æ®µ"])
        for idx, item in enumerate(todos, 1):
            writer.writerow([
                idx,
                item["title"],
                item["type"],
                item["url"],
                "verified_by, compliance_levelï¼ˆå‚è€ƒstatic/meta/usage_guide.jsonï¼‰"
            ])
    
    return csv_path, len(todos)

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("AI-Vent-Share åŒ¿ååŒ–ä¼˜åŒ–è„šæœ¬")
    print("="*70)
    print("ğŸ”’ åŒ¿åæ€§æœ€é«˜å‡†åˆ™ï¼šå½»åº•ç§»é™¤æ‰€æœ‰IDå­—æ®µï¼ˆéæ›¿æ¢/ä¼ªè£…ï¼‰")
    print("="*70)
    
    # æ­¥éª¤1ï¼šæ¸…ç©ºæ—§åˆ†ç‰‡
    clean_old_shards()
    
    # æ­¥éª¤2ï¼šè¯»å–ä¸»ç´¢å¼• + åŒ¿ååŒ–æ¸…ç†
    data, cleaned_resources = load_and_anonymize()
    total = len(cleaned_resources)
    
    # æ­¥éª¤3ï¼šæ³¨å…¥å¯ä¿¡åº¦æ¡†æ¶
    print(f"\nğŸ”§ æ³¨å…¥å¯ä¿¡åº¦æ¡†æ¶...")
    enhanced, todos = inject_credibility_framework(cleaned_resources)
    print(f"  âœ“ å·²æ³¨å…¥ {len(enhanced)} æ¡èµ„æº")
    
    # æ­¥éª¤4ï¼šæ›´æ–°ä¸»ç´¢å¼•
    print(f"\nğŸ’¾ æ›´æ–°ä¸»ç´¢å¼•ï¼ˆè¦†ç›–å†™å…¥åŒ¿ååŒ–+å¢å¼ºç‰ˆï¼‰...")
    save_main_index(data, enhanced)
    print(f"  âœ“ ä¸»ç´¢å¼•å·²æ›´æ–°")
    
    # æ­¥éª¤5ï¼šç”Ÿæˆåˆ†ç‰‡
    print(f"\nğŸ“¦ ç”Ÿæˆåˆ†ç‰‡ç´¢å¼•ï¼ˆçº¯å‡€æ–‡ä»¶ï¼Œæ— ä»»ä½•IDæ®‹ç•™ï¼‰...")
    valid_files = generate_shards(enhanced)
    
    # æ­¥éª¤6ï¼šç”ŸæˆéªŒè¯æ¸…å•
    print(f"\nğŸ“‹ ç”ŸæˆéªŒè¯æ¸…å•...")
    summary_path = generate_summary(total, valid_files)
    print(f"  âœ“ éªŒè¯æ¸…å•: {summary_path}")
    
    # æ­¥éª¤7ï¼šç”Ÿæˆå¾…åŠæ¸…å•
    csv_path, todo_count = generate_todo_csv(todos)
    print(f"  âœ“ å¾…åŠæ¸…å•: {csv_path} ({todo_count}æ¡å¾…è¡¥å……)")
    
    # è¾“å‡ºæŠ¥å‘Š
    print("\n" + "="*70)
    print(f"âœ¨ åŒ¿åä¼˜åŒ–å®Œæˆï¼èµ„æºæ€»é‡: {total}æ¡ | åˆ†ç±»: {len(valid_files)}ç±»")
    print("="*70)
    print("âœ… åŒ¿åæ€§å¼ºåŒ–: å½»åº•ç§»é™¤æ‰€æœ‰èµ„æºçš„ID/submitter/timestampç­‰å­—æ®µ")
    print(f"âœ… å¯ä¿¡åº¦æ¡†æ¶: å·²æ³¨å…¥ï¼ˆç©ºå€¼å®‰å…¨ï½œAIå¯ç­›é€‰ï¼‰")
    print(f"âœ… åˆ†ç‰‡ç´¢å¼•: ç”Ÿæˆ {len(valid_files)} ä¸ªçº¯å‡€æ–‡ä»¶ï¼ˆæ— ä»»ä½•IDæ®‹ç•™ï¼‰")
    print(f"âœ… å¾…åŠæ¸…å•: {csv_path}ï¼ˆç”¨æ ‡é¢˜+URLå®šä½ï¼Œæ— IDï¼‰")
    print(f"âœ… éªŒè¯æ–‡ä»¶: {summary_path}ï¼ˆå«åŒ¿åæ€§å£°æ˜ï¼‰")
    print("="*70)
    print("\nğŸ’¡ äººç±»ç»´æŠ¤è€…æ“ä½œæŒ‡å—:")
    print("  1. è¡¥å……å¯ä¿¡åº¦ä¿¡æ¯æ—¶ï¼Œè¯·é€šè¿‡ã€Œæ ‡é¢˜+URLã€åœ¨content_index.jsonä¸­å®šä½èµ„æº")
    print("  2. æ·»åŠ æ–°èµ„æºæ—¶ï¼Œè¯·å‹¿åœ¨manual_sync.pyä¸­ç”Ÿæˆid/submitterç­‰å­—æ®µ")
    print("  3. è¿è¡Œæœ¬è„šæœ¬åï¼Œæ‰€æœ‰å…¬å¼€JSONå·²100%åŒ¿åï¼ˆGitæäº¤å³ç”Ÿæ•ˆï¼‰")
    print("  4. æ— éœ€æ‹…å¿ƒèµ„æºæ›´æ–°ï¼šé€šè¿‡æ ‡é¢˜+URL+åˆ†ç±»å³å¯ç²¾å‡†å®šä½")
    print("\n")

if __name__ == "__main__":
    main()
