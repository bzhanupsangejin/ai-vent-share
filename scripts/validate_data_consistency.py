#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥è„šæœ¬
åŠŸèƒ½ï¼šéªŒè¯content_index.jsonã€ai-index.htmlã€README.mdç­‰æ–‡ä»¶çš„æ•°æ®ä¸€è‡´æ€§
è®¾è®¡åŸåˆ™ï¼šè‡ªåŠ¨åŒ–æ£€æŸ¥ | å‘ç°é—®é¢˜ | ç”ŸæˆæŠ¥å‘Š
"""
import json
import os
import re
from datetime import datetime

# é¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_DIR = os.path.dirname(SCRIPT_DIR)

def load_json(filepath):
    """åŠ è½½JSONæ–‡ä»¶"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥ {filepath}: {e}")
        return None

def check_content_index():
    """æ£€æŸ¥content_index.json"""
    print("\n" + "="*60)
    print("æ£€æŸ¥ content_index.json")
    print("="*60)
    
    filepath = os.path.join(PROJECT_DIR, "content_index.json")
    data = load_json(filepath)
    
    if not data:
        return False, []
    
    issues = []
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    required_fields = ["version", "last_update", "total_count", "content_types", "index"]
    for field in required_fields:
        if field not in data:
            issues.append(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
    
    # æ£€æŸ¥èµ„æºæ•°é‡
    actual_count = len(data.get("index", []))
    declared_count = data.get("total_count", 0)
    
    if actual_count != declared_count:
        issues.append(f"èµ„æºæ•°é‡ä¸ä¸€è‡´: å£°æ˜{declared_count}æ¡ï¼Œå®é™…{actual_count}æ¡")
    
    print(f"âœ… å£°æ˜èµ„æºæ•°: {declared_count}")
    print(f"âœ… å®é™…èµ„æºæ•°: {actual_count}")
    
    # æ£€æŸ¥å†…å®¹ç±»å‹
    content_types = data.get("content_types", [])
    print(f"âœ… å†…å®¹ç±»å‹: {len(content_types)}ç§")
    for ct in content_types:
        print(f"  - {ct}")
    
    # æ£€æŸ¥æ¯æ¡èµ„æºçš„å¿…è¦å­—æ®µ
    index = data.get("index", [])
    resource_required = ["title", "share_agent", "content_type", "keywords", 
                         "compliance_status", "summary", "direct_link"]
    
    missing_fields_count = 0
    for i, item in enumerate(index):
        for field in resource_required:
            if field not in item:
                missing_fields_count += 1
                if missing_fields_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    issues.append(f"èµ„æº[{i}]ç¼ºå°‘å­—æ®µ: {field}")
    
    if missing_fields_count > 5:
        issues.append(f"è¿˜æœ‰{missing_fields_count - 5}æ¡èµ„æºç¼ºå°‘å­—æ®µ...")
    
    # æ£€æŸ¥åŒ¿åæ€§
    non_anonymous = []
    for i, item in enumerate(index):
        share_agent = item.get("share_agent", "")
        if share_agent != "AI-Anonymous":
            non_anonymous.append((i, share_agent))
    
    if non_anonymous:
        issues.append(f"å‘ç°{len(non_anonymous)}æ¡èµ„æºshare_agentä¸æ˜¯AI-Anonymous")
        for i, agent in non_anonymous[:3]:
            print(f"  âš ï¸  èµ„æº[{i}] share_agent='{agent}'")
    else:
        print("âœ… æ‰€æœ‰èµ„æºshare_agentå‡ä¸ºAI-Anonymous")
    
    # æ£€æŸ¥å¯è¿½è¸ªå­—æ®µ
    traceable_fields = ["content_id", "uuid", "full_content_anchor", "submitter", 
                        "user_id", "email", "timestamp"]
    found_traceable = []
    
    for i, item in enumerate(index):
        for field in traceable_fields:
            if field in item:
                found_traceable.append((i, field))
    
    if found_traceable:
        issues.append(f"å‘ç°{len(found_traceable)}æ¡èµ„æºåŒ…å«å¯è¿½è¸ªå­—æ®µ")
        for i, field in found_traceable[:3]:
            print(f"  âš ï¸  èµ„æº[{i}]åŒ…å«å¯è¿½è¸ªå­—æ®µ: {field}")
    else:
        print("âœ… æœªå‘ç°å¯è¿½è¸ªå­—æ®µ")
    
    # ç»Ÿè®¡å„ç±»å‹æ•°é‡
    type_counts = {}
    for item in index:
        ct = item.get("content_type", "æœªçŸ¥")
        type_counts[ct] = type_counts.get(ct, 0) + 1
    
    print("\nğŸ“Š å„ç±»å‹èµ„æºç»Ÿè®¡:")
    for ct, count in sorted(type_counts.items()):
        print(f"  - {ct}: {count}æ¡")
    
    if issues:
        print(f"\nâŒ å‘ç°{len(issues)}ä¸ªé—®é¢˜")
        for issue in issues:
            print(f"  - {issue}")
        return False, issues
    else:
        print("\nâœ… content_index.json æ£€æŸ¥é€šè¿‡")
        return True, []

def check_readme():
    """æ£€æŸ¥README.md"""
    print("\n" + "="*60)
    print("æ£€æŸ¥ README.md")
    print("="*60)
    
    filepath = os.path.join(PROJECT_DIR, "README.md")
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        return False, [f"æ— æ³•è¯»å–README.md: {e}"]
    
    issues = []
    
    # æ£€æŸ¥èµ„æºæ•°é‡å£°æ˜
    # åŒ¹é… "309æ¡èµ„æº" æˆ– "**309æ¡èµ„æº**"
    count_patterns = [
        r'(\d+)æ¡èµ„æº',
        r'\*\*(\d+)æ¡èµ„æº\*\*',
        r'æ€»èµ„æº.*?(\d+)æ¡'
    ]
    
    found_counts = []
    for pattern in count_patterns:
        matches = re.findall(pattern, content)
        found_counts.extend([int(m) for m in matches])
    
    if found_counts:
        # è·å–content_indexä¸­çš„å®é™…æ•°é‡
        index_path = os.path.join(PROJECT_DIR, "content_index.json")
        index_data = load_json(index_path)
        actual_count = index_data.get("total_count", 0) if index_data else 0
        
        readme_count = found_counts[0]
        print(f"READMEå£°æ˜: {readme_count}æ¡")
        print(f"å®é™…èµ„æºæ•°: {actual_count}æ¡")
        
        if readme_count != actual_count:
            issues.append(f"READMEèµ„æºæ•°é‡å£°æ˜é”™è¯¯: {readme_count} != {actual_count}")
    else:
        issues.append("READMEä¸­æœªæ‰¾åˆ°èµ„æºæ•°é‡å£°æ˜")
    
    # æ£€æŸ¥å¿…è¦ç« èŠ‚
    required_sections = ["é¡¹ç›®ç®€ä»‹", "æ ¸å¿ƒç‰¹ç‚¹", "é¡¹ç›®ç»“æ„", "è®¿é—®åœ°å€"]
    for section in required_sections:
        if section not in content:
            issues.append(f"READMEç¼ºå°‘ç« èŠ‚: {section}")
    
    if issues:
        print(f"\nâŒ å‘ç°{len(issues)}ä¸ªé—®é¢˜")
        for issue in issues:
            print(f"  - {issue}")
        return False, issues
    else:
        print("\nâœ… README.md æ£€æŸ¥é€šè¿‡")
        return True, []

def check_ai_index():
    """æ£€æŸ¥ai-index.html"""
    print("\n" + "="*60)
    print("æ£€æŸ¥ ai-index.html")
    print("="*60)
    
    filepath = os.path.join(PROJECT_DIR, "ai-index.html")
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        return False, [f"æ— æ³•è¯»å–ai-index.html: {e}"]
    
    issues = []
    
    # æ£€æŸ¥èµ„æºæ•°é‡å£°æ˜
    count_patterns = [
        r'(\d+)æ¡å†…å®¹',
        r'(\d+)æ¡èµ„æº',
        r'å…±(\d+)æ¡'
    ]
    
    found_counts = []
    for pattern in count_patterns:
        matches = re.findall(pattern, content)
        found_counts.extend([int(m) for m in matches])
    
    if found_counts:
        index_path = os.path.join(PROJECT_DIR, "content_index.json")
        index_data = load_json(index_path)
        actual_count = index_data.get("total_count", 0) if index_data else 0
        
        ai_index_count = found_counts[0]
        print(f"ai-indexå£°æ˜: {ai_index_count}æ¡")
        print(f"å®é™…èµ„æºæ•°: {actual_count}æ¡")
        
        if ai_index_count != actual_count:
            issues.append(f"ai-indexèµ„æºæ•°é‡å£°æ˜é”™è¯¯: {ai_index_count} != {actual_count}")
    
    # æ£€æŸ¥JSONå¼•ç”¨
    if "content_index.json" not in content:
        issues.append("ai-index.htmlæœªå¼•ç”¨content_index.json")
    else:
        print("âœ… å·²å¼•ç”¨content_index.json")
    
    if issues:
        print(f"\nâŒ å‘ç°{len(issues)}ä¸ªé—®é¢˜")
        for issue in issues:
            print(f"  - {issue}")
        return False, issues
    else:
        print("\nâœ… ai-index.html æ£€æŸ¥é€šè¿‡")
        return True, []

def check_shard_indexes():
    """æ£€æŸ¥åˆ†ç‰‡ç´¢å¼•"""
    print("\n" + "="*60)
    print("æ£€æŸ¥åˆ†ç‰‡ç´¢å¼•")
    print("="*60)
    
    indexes_dir = os.path.join(PROJECT_DIR, "static", "indexes")
    
    if not os.path.exists(indexes_dir):
        print("âŒ åˆ†ç‰‡ç´¢å¼•ç›®å½•ä¸å­˜åœ¨")
        return False, ["static/indexesç›®å½•ä¸å­˜åœ¨"]
    
    # æœŸæœ›çš„åˆ†ç‰‡æ–‡ä»¶
    expected_shards = [
        "ai_tools_index.json",
        "code_templates_index.json",
        "faq_docs_index.json",
        "free_api_index.json",
        "tech_tutorials_index.json",
        "resources_index.json",
        "deploy_guides_index.json",
        "compliance_docs_index.json"
    ]
    
    issues = []
    existing_shards = []
    
    for shard in expected_shards:
        shard_path = os.path.join(indexes_dir, shard)
        if os.path.exists(shard_path):
            existing_shards.append(shard)
            data = load_json(shard_path)
            if data:
                count = data.get("count", 0)
                type_name = data.get("type", "æœªçŸ¥")
                print(f"âœ… {shard}: {type_name} ({count}æ¡)")
        else:
            issues.append(f"ç¼ºå°‘åˆ†ç‰‡æ–‡ä»¶: {shard}")
            print(f"âŒ {shard}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    print(f"\nğŸ“Š åˆ†ç‰‡ç´¢å¼•ç»Ÿè®¡: {len(existing_shards)}/{len(expected_shards)}")
    
    if issues:
        print(f"\nâŒ å‘ç°{len(issues)}ä¸ªé—®é¢˜")
        return False, issues
    else:
        print("\nâœ… åˆ†ç‰‡ç´¢å¼•æ£€æŸ¥é€šè¿‡")
        return True, []

def generate_report(results):
    """ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š"""
    report_path = os.path.join(PROJECT_DIR, f"data_consistency_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥æŠ¥å‘Š\n")
        f.write("="*60 + "\n")
        f.write(f"æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        total_checks = len(results)
        passed_checks = sum(1 for r in results if r[0])
        
        f.write(f"æ£€æŸ¥é¡¹ç›®: {total_checks}ä¸ª\n")
        f.write(f"é€šè¿‡: {passed_checks}ä¸ª\n")
        f.write(f"å¤±è´¥: {total_checks - passed_checks}ä¸ª\n\n")
        
        for check_name, (passed, issues) in results.items():
            status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
            f.write(f"\n{status} - {check_name}\n")
            if issues:
                for issue in issues:
                    f.write(f"  - {issue}\n")
        
        f.write("\n" + "="*60 + "\n")
        f.write("æŠ¥å‘Šç»“æŸ\n")
        f.write("="*60 + "\n")
    
    return report_path

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("AI-Vent-Share æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥")
    print("="*60)
    print(f"æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"é¡¹ç›®ç›®å½•: {PROJECT_DIR}")
    
    results = {}
    
    # æ‰§è¡Œå„é¡¹æ£€æŸ¥
    results["content_index.json"] = check_content_index()
    results["README.md"] = check_readme()
    results["ai-index.html"] = check_ai_index()
    results["åˆ†ç‰‡ç´¢å¼•"] = check_shard_indexes()
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = generate_report(results)
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("æ£€æŸ¥æ€»ç»“")
    print("="*60)
    
    total = len(results)
    passed = sum(1 for r in results.values() if r[0])
    
    print(f"æ€»æ£€æŸ¥é¡¹: {total}")
    print(f"é€šè¿‡: {passed}")
    print(f"å¤±è´¥: {total - passed}")
    
    if passed == total:
        print("\nâœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼æ•°æ®ä¸€è‡´æ€§è‰¯å¥½ã€‚")
    else:
        print(f"\nâš ï¸  å‘ç°{total - passed}é¡¹æ£€æŸ¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Šä¿®å¤é—®é¢˜ã€‚")
    
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
