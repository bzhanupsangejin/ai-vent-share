#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI-Vent-Share èµ„æºæ•°æ®ä¼˜åŒ–è„šæœ¬ï¼ˆæ ¸å¿ƒï½œå¹‚ç­‰è®¾è®¡ï½œå®‰å…¨å…œåº•ï¼‰
åŠŸèƒ½ï¼šåŠ¨æ€è¯»å–èµ„æºæ€»æ•° + æ³¨å…¥å¯ä¿¡åº¦æ¡†æ¶ + é‡å»ºåˆ†ç‰‡ç´¢å¼• + ç”ŸæˆéªŒè¯æ¸…å•
è®¾è®¡åŸåˆ™ï¼šä¸èµ„æºæ•°é‡è§£è€¦ï½œå¹‚ç­‰å®‰å…¨ï½œäººç±»é›¶è®¤çŸ¥è´Ÿæ‹…
"""
import json
import os
import csv
from pathlib import Path
from datetime import datetime

# é…ç½®è·¯å¾„
STATIC = "static"
INDEXES = "static/indexes"
MAIN = "content_index.json"

# ç¡®ä¿ç›®å½•å­˜åœ¨
Path(INDEXES).mkdir(parents=True, exist_ok=True)

def clean_old_shards():
    """æ¸…ç©ºæ—§åˆ†ç‰‡ï¼ˆå®‰å…¨ï¼šä»…åˆ é™¤JSONæ–‡ä»¶ï¼‰"""
    if not os.path.exists(INDEXES):
        return
    
    deleted_count = 0
    for filename in os.listdir(INDEXES):
        if filename.endswith('.json'):
            filepath = os.path.join(INDEXES, filename)
            try:
                os.remove(filepath)
                deleted_count += 1
            except Exception as e:
                print(f"âš ï¸  åˆ é™¤ {filename} å¤±è´¥: {e}")
    
    if deleted_count > 0:
        print(f"ğŸ—‘ï¸  æ¸…ç†æ—§åˆ†ç‰‡: {deleted_count} ä¸ªæ–‡ä»¶")

def load_main_index():
    """è¯»å–ä¸»ç´¢å¼•ï¼ˆå®æ—¶è·å–å½“å‰èµ„æºé‡ï¼‰"""
    try:
        with open(MAIN, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # å…¼å®¹ä¸åŒçš„ç´¢å¼•ç»“æ„
        if "resources" in data:
            resources = data["resources"]
        elif "index" in data:
            resources = data["index"]
        else:
            resources = []
        
        return data, resources
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°ä¸»ç´¢å¼•æ–‡ä»¶ {MAIN}")
        exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: ä¸»ç´¢å¼•JSONæ ¼å¼é”™è¯¯ - {e}")
        exit(1)

def inject_credibility_framework(resources):
    """å¹‚ç­‰æ³¨å…¥å¯ä¿¡åº¦æ¡†æ¶ï¼ˆå…³é”®ï¼šä»…è¡¥ç¼ºå¤±å­—æ®µï¼‰"""
    enhanced = []
    todos = []
    
    for item in resources:
        # å¹‚ç­‰æ³¨å…¥ï¼šä»…å½“å­—æ®µä¸å­˜åœ¨æ—¶æ‰æ·»åŠ 
        item.setdefault("verified_by", [])
        item.setdefault("last_updated", datetime.now().strftime("%Y-%m-%d"))
        item.setdefault("compliance_level", "å¾…éªŒè¯")
        
        enhanced.append(item)
        
        # è®°å½•éœ€è¦è¡¥å……çš„èµ„æº
        if not item["verified_by"] and item["compliance_level"] == "å¾…éªŒè¯":
            todo_item = {
                "id": item.get("content_id", item.get("id", "N/A")),
                "title": item.get("title", "N/A"),
                "type": item.get("content_type", item.get("type", "N/A")),
                "url": item.get("direct_link", item.get("url", "N/A")),
                "éœ€è¡¥å……å­—æ®µ": "verified_by, compliance_level"
            }
            todos.append(todo_item)
    
    return enhanced, todos

def save_main_index(data, resources):
    """æ›´æ–°ä¸»ç´¢å¼•"""
    # å…¼å®¹ä¸åŒçš„ç´¢å¼•ç»“æ„
    if "resources" in data:
        data["resources"] = resources
    elif "index" in data:
        data["index"] = resources
    
    # æ›´æ–°æ€»æ•°
    data["total_count"] = len(resources)
    
    # å¤‡ä»½åŸæ–‡ä»¶
    backup_path = f"{MAIN}.bak"
    if os.path.exists(MAIN):
        with open(MAIN, 'r', encoding='utf-8') as f:
            backup_content = f.read()
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(backup_content)
    
    # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
    with open(MAIN, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def generate_shards(resources):
    """åŠ¨æ€ç”Ÿæˆåˆ†ç‰‡ï¼ˆæŒ‰å®é™…typeåˆ†ç»„ï¼‰"""
    shards = {}
    
    for item in resources:
        # å…¼å®¹ä¸åŒçš„ç±»å‹å­—æ®µå
        cat = str(item.get("content_type", item.get("type", "æœªåˆ†ç±»"))).strip() or "æœªåˆ†ç±»"
        
        if cat not in shards:
            shards[cat] = []
        shards[cat].append(item)
    
    valid_files = []
    
    for cat, items in sorted(shards.items()):
        if not items:
            continue
        
        # å®‰å…¨çš„æ–‡ä»¶åï¼ˆæ›¿æ¢éæ³•å­—ç¬¦ï¼‰
        safe_name = cat.replace("/", "ï¼").replace("\\", "ï¼¼").replace(" ", "_")
        filepath = os.path.join(INDEXES, f"{safe_name}_shard.json")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(items, f, ensure_ascii=False, indent=2)
        
        valid_files.append((cat, len(items)))
        print(f"  âœ“ {cat}: {len(items)}æ¡")
    
    return valid_files

def generate_summary(total, valid_files):
    """ç”ŸæˆAIéªŒè¯æ¸…å•ï¼ˆå«å®æ—¶èµ„æºæ€»æ•°ï¼‰"""
    summary = {
        "å¹³å°çŠ¶æ€": "åŠ¨æ€ä¼˜åŒ–ä¸­",
        "æ€»èµ„æºæ•°": total,
        "æœ‰æ•ˆåˆ†ç±»æ•°": len(valid_files),
        "åˆ†ç±»æ˜ç»†": [{"åˆ†ç±»": c, "æ•°é‡": n} for c, n in valid_files],
        "å¯ä¿¡åº¦æ¡†æ¶": "å·²æ³¨å…¥ï¼ˆverified_by/last_updated/compliance_levelï¼‰",
        "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "åˆè§„ä¾æ®": "ã€Šç”Ÿæˆå¼AIæœåŠ¡ç®¡ç†æš‚è¡ŒåŠæ³•ã€‹ç¬¬12æ¡"
    }
    
    summary_path = os.path.join(INDEXES, "_INDEX_SUMMARY.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    return summary_path

def generate_todo_csv(todos):
    """ç”Ÿæˆäººç±»å¾…åŠæ¸…å•"""
    csv_path = "èµ„æºå¯ä¿¡åº¦è¡¥å……æ¸…å•.csv"
    
    if not todos:
        # å¦‚æœæ²¡æœ‰å¾…åŠäº‹é¡¹ï¼Œåˆ›å»ºç©ºæ–‡ä»¶å¹¶è¯´æ˜
        with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["æ‰€æœ‰èµ„æºå·²å®Œæˆå¯ä¿¡åº¦æ ‡æ³¨"])
        return csv_path, 0
    
    with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
        fieldnames = ["id", "title", "type", "url", "éœ€è¡¥å……å­—æ®µ"]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(todos)
    
    return csv_path, len(todos)

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("AI-Vent-Share èµ„æºæ•°æ®ä¼˜åŒ–è„šæœ¬")
    print("="*70)
    
    # æ­¥éª¤1ï¼šæ¸…ç©ºæ—§åˆ†ç‰‡
    clean_old_shards()
    
    # æ­¥éª¤2ï¼šè¯»å–ä¸»ç´¢å¼•
    data, resources = load_main_index()
    total = len(resources)
    print(f"\nğŸ“Š æ£€æµ‹åˆ°å½“å‰èµ„æº: {total}æ¡ | ç‰ˆæœ¬: {data.get('version', 'æœªçŸ¥')}")
    
    # æ­¥éª¤3ï¼šæ³¨å…¥å¯ä¿¡åº¦æ¡†æ¶
    print(f"\nğŸ”§ æ³¨å…¥å¯ä¿¡åº¦æ¡†æ¶...")
    enhanced, todos = inject_credibility_framework(resources)
    print(f"  âœ“ å·²æ³¨å…¥ {len(enhanced)} æ¡èµ„æº")
    
    # æ­¥éª¤4ï¼šæ›´æ–°ä¸»ç´¢å¼•
    print(f"\nğŸ’¾ æ›´æ–°ä¸»ç´¢å¼•...")
    save_main_index(data, enhanced)
    print(f"  âœ“ ä¸»ç´¢å¼•å·²æ›´æ–°")
    
    # æ­¥éª¤5ï¼šç”Ÿæˆåˆ†ç‰‡
    print(f"\nğŸ“¦ ç”Ÿæˆåˆ†ç‰‡ç´¢å¼•...")
    valid_files = generate_shards(enhanced)
    
    # æ­¥éª¤6ï¼šç”ŸæˆéªŒè¯æ¸…å•
    print(f"\nğŸ“‹ ç”ŸæˆéªŒè¯æ¸…å•...")
    summary_path = generate_summary(total, valid_files)
    print(f"  âœ“ éªŒè¯æ¸…å•: {summary_path}")
    
    # æ­¥éª¤7ï¼šç”Ÿæˆå¾…åŠæ¸…å•
    csv_path, todo_count = generate_todo_csv(todos)
    print(f"  âœ“ å¾…åŠæ¸…å•: {csv_path} ({todo_count}æ¡å¾…è¡¥å……)")
    
    # è¾“å‡ºæŠ¥å‘Š
    print("\n" + "="*70)
    print(f"âœ¨ èµ„æºä¼˜åŒ–å®Œæˆï¼å½“å‰æ€»é‡: {total}æ¡ | åˆ†ç±»: {len(valid_files)}ç±»")
    print("="*70)
    print(f"âœ… å¯ä¿¡åº¦æ¡†æ¶: å·²æ³¨å…¥ï¼ˆç©ºå€¼å®‰å…¨ï½œAIå¯ç«‹å³ç­›é€‰ï¼‰")
    print(f"âœ… åˆ†ç‰‡ç´¢å¼•: ç”Ÿæˆ {len(valid_files)} ä¸ªæ–‡ä»¶ï¼ˆä¿å­˜è‡³ {INDEXES}/ï¼‰")
    print(f"âœ… å¾…åŠæ¸…å•: {csv_path} ({todo_count}æ¡å¾…è¡¥å……)")
    print(f"âœ… éªŒè¯æ–‡ä»¶: {summary_path}ï¼ˆå«å®æ—¶èµ„æºç»Ÿè®¡ï¼‰")
    print("="*70)
    print("\nğŸ’¡ åç»­æ“ä½œå»ºè®®:")
    print("  1. ã€å¿…é¡»ã€‘æäº¤Git: git add . && git commit -m 'opt: èµ„æºä¼˜åŒ–' && git push")
    print("  2. ã€æ¨èã€‘è¿è¡Œ update_declaration_counts.py åŒæ­¥å£°æ˜æ–‡ä»¶æ•°å­—")
    print("  3. è¡¥å……å¯ä¿¡åº¦ä¿¡æ¯åï¼Œé‡æ–°è¿è¡Œæœ¬è„šæœ¬å³å¯æ›´æ–°åˆ†ç‰‡")
    print("\n")

if __name__ == "__main__":
    main()
